{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import einops\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Embedding, ReLU, Dropout, Bidirectional, GRU, Add, Concatenate\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\salha\\\\studium\\\\NLP\\\\projektabeit_nlp_2\\\\analyse_phase_01'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join('data', 'choc_recipes.txt')\n",
    "# Using readlines()\n",
    "file1 = open(text_path, 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "names = []\n",
    "ingredients = []\n",
    "steps = []\n",
    "\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    pieces = line.split('\\t')\n",
    "    names.append(pieces[0])\n",
    "    ingredients.append(pieces[1])\n",
    "    steps.append(pieces[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name count: 7404\n",
      "Ingredient count: 7404\n",
      "Step count: 7404\n"
     ]
    }
   ],
   "source": [
    "print(f'Name count: {len(names)}')\n",
    "print(f'Ingredient count: {len(ingredients)}')\n",
    "print(f'Step count: {len(steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "\n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_list_list = []\n",
    "for i_string in ingredients:\n",
    "    i = i_string.replace(', ', '*')\n",
    "    i = i.replace(' ', '_')\n",
    "    i = i.replace('*', ', ')\n",
    "\n",
    "    ingredients_list_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ingredients_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "name_input = np.array(names)\n",
    "ingredient_input = np.array(ingredients)\n",
    "step_output = np.array(steps)\n",
    "\n",
    "ingredient_dataset = tf.data.Dataset.from_tensor_slices(ingredient_input)\n",
    "name_dataset =  tf.data.Dataset.from_tensor_slices(name_input)\n",
    "step_dataset =  tf.data.Dataset.from_tensor_slices(step_output)\n",
    "#dataset_12 = tf.data.Dataset.from_tensor_slices((name_input, ingredient_input))\n",
    "#dataset_label = tf.data.Dataset.from_tensor_slices(step_output)\n",
    "\n",
    "#dataset = tf.data.Dataset.zip((dataset_12, dataset_label)).batch(64)\n",
    "#model.fit(dataset, epochs=10, steps_per_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 400\n",
    "\n",
    "\n",
    "step_vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "step_vectorize_layer.adapt(step_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "max_len = 4\n",
    "\n",
    "\n",
    "ingredient_vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "\n",
    "ingredient_vectorize_layer.adapt(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 4\n",
    "\n",
    "\n",
    "name_vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "name_vectorize_layer.adapt(name_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'chocolate',\n",
       " 'cake',\n",
       " 'cookies',\n",
       " 'chip',\n",
       " 's',\n",
       " 'brownies',\n",
       " 'fudge',\n",
       " 'butter',\n",
       " 'peanut',\n",
       " 'bars',\n",
       " 'cream',\n",
       " 'pie',\n",
       " 'with',\n",
       " 'and',\n",
       " 'white',\n",
       " 'hot',\n",
       " 'pudding',\n",
       " 'brownie',\n",
       " 'muffins',\n",
       " 'easy',\n",
       " 'no',\n",
       " 'cheesecake',\n",
       " 'oatmeal',\n",
       " 'banana',\n",
       " 'double',\n",
       " 'mocha',\n",
       " 'cocoa',\n",
       " 'cookie',\n",
       " 'cupcakes',\n",
       " 'caramel',\n",
       " 'bread',\n",
       " 'the',\n",
       " 'free',\n",
       " 'cherry',\n",
       " 'low',\n",
       " 'bake',\n",
       " 'orange',\n",
       " 'mousse',\n",
       " 'frosting',\n",
       " 'coffee',\n",
       " 'ice',\n",
       " 'fat',\n",
       " 'squares',\n",
       " 'sauce',\n",
       " 'coconut',\n",
       " 'almond',\n",
       " 'best',\n",
       " 'dark',\n",
       " 'mix',\n",
       " 'dessert',\n",
       " 'raspberry',\n",
       " 'mint',\n",
       " 'toffee',\n",
       " 'chewy',\n",
       " 'cheese',\n",
       " 'pecan',\n",
       " 'vegan',\n",
       " 'for',\n",
       " 'black',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'nut',\n",
       " 'truffles',\n",
       " 'balls',\n",
       " 'fudgy',\n",
       " 'candy',\n",
       " 'layer',\n",
       " 'mexican',\n",
       " 'triple',\n",
       " 'or',\n",
       " 'biscotti',\n",
       " 'walnut',\n",
       " 'of',\n",
       " 'chunk',\n",
       " 'bar',\n",
       " 'cranberry',\n",
       " 'cinnamon',\n",
       " 'in',\n",
       " 'decadent',\n",
       " 'bundt',\n",
       " 'kahlua',\n",
       " 'milk',\n",
       " 'creamy',\n",
       " 'cakes',\n",
       " 'marshmallow',\n",
       " 'espresso',\n",
       " 'sugar',\n",
       " 'oreo',\n",
       " 'torte',\n",
       " 'pumpkin',\n",
       " 'gluten',\n",
       " 'strawberry',\n",
       " 'one',\n",
       " 'icing',\n",
       " 'mini',\n",
       " 'frozen',\n",
       " 'zucchini',\n",
       " 'peppermint',\n",
       " 'flourless',\n",
       " 'cappuccino',\n",
       " 'nutella',\n",
       " 'pound',\n",
       " 'macadamia',\n",
       " 'truffle',\n",
       " 'm',\n",
       " 'hazelnut',\n",
       " 'irish',\n",
       " 'ever',\n",
       " 'covered',\n",
       " 'trifle',\n",
       " 'mud',\n",
       " 'sweet',\n",
       " 'sour',\n",
       " 'hershey',\n",
       " 'german',\n",
       " 'french',\n",
       " 'choc',\n",
       " 'ultimate',\n",
       " 'healthy',\n",
       " 'forest',\n",
       " 'slice',\n",
       " 'pot',\n",
       " 'super',\n",
       " 'minute',\n",
       " 'microwave',\n",
       " 'homemade',\n",
       " 'filled',\n",
       " 'delicious',\n",
       " 'de',\n",
       " 'crunch',\n",
       " 'shortbread',\n",
       " 'my',\n",
       " 'gooey',\n",
       " 'favorite',\n",
       " 'bowl',\n",
       " 'red',\n",
       " 'moist',\n",
       " 'dipped',\n",
       " 'christmas',\n",
       " 'spiced',\n",
       " 'scones',\n",
       " 'rum',\n",
       " 'rice',\n",
       " 'oat',\n",
       " 'food',\n",
       " 'yummy',\n",
       " 'turtle',\n",
       " 'swirl',\n",
       " 'recipe',\n",
       " 'quick',\n",
       " 'pancakes',\n",
       " 'bites',\n",
       " 'warm',\n",
       " 'mom',\n",
       " 'house',\n",
       " 'cups',\n",
       " 'crock',\n",
       " 'wheat',\n",
       " 'tart',\n",
       " 'old',\n",
       " 'from',\n",
       " 'delight',\n",
       " 'cup',\n",
       " 'velvet',\n",
       " 'treats',\n",
       " 'soft',\n",
       " 'rocky',\n",
       " 'mores',\n",
       " 'molten',\n",
       " 'choco',\n",
       " 'carb',\n",
       " 'road',\n",
       " 'krispies',\n",
       " 'ganache',\n",
       " 'dairy',\n",
       " 'by',\n",
       " 'sheet',\n",
       " 'raisin',\n",
       " 'light',\n",
       " 'buttermilk',\n",
       " 'blondies',\n",
       " 'snack',\n",
       " 'glaze',\n",
       " 'copycat',\n",
       " 'butterscotch',\n",
       " 'tiramisu',\n",
       " 'grandma',\n",
       " 'good',\n",
       " 'fondue',\n",
       " 'vanilla',\n",
       " 'surprise',\n",
       " 'style',\n",
       " 'n',\n",
       " 'more',\n",
       " 'marble',\n",
       " 'frosted',\n",
       " 'dough',\n",
       " 'crispy',\n",
       " 'amaretto',\n",
       " 'syrup',\n",
       " 'pizza',\n",
       " 'loaf',\n",
       " 'lemon',\n",
       " 'layered',\n",
       " 'devil',\n",
       " 'toll',\n",
       " 'to',\n",
       " 'spice',\n",
       " 'macaroons',\n",
       " 'kiss',\n",
       " 'holiday',\n",
       " 'graham',\n",
       " 'filling',\n",
       " 'easter',\n",
       " 'apricot',\n",
       " '5',\n",
       " '2',\n",
       " 'whole',\n",
       " 'meringue',\n",
       " 'lava',\n",
       " 'kisses',\n",
       " 'honey',\n",
       " 'fashioned',\n",
       " 'eggs',\n",
       " 'egg',\n",
       " 'deep',\n",
       " 'deen',\n",
       " 'creme',\n",
       " 'classic',\n",
       " 'big',\n",
       " 'amazing',\n",
       " '1',\n",
       " 'world',\n",
       " 'reese',\n",
       " 'paula',\n",
       " 'pan',\n",
       " 'marbled',\n",
       " 'chips',\n",
       " 'buttercream',\n",
       " 'bottom',\n",
       " 'your',\n",
       " 'ww',\n",
       " 'topped',\n",
       " 'streusel',\n",
       " 'sandwich',\n",
       " 'passover',\n",
       " 'mug',\n",
       " 'magic',\n",
       " 'iced',\n",
       " 'halloween',\n",
       " 'drops',\n",
       " 'down',\n",
       " 'crunchy',\n",
       " 'bourbon',\n",
       " '3',\n",
       " 'waffles',\n",
       " 'spicy',\n",
       " 'simple',\n",
       " 'pistachio',\n",
       " 'mrs',\n",
       " 'maple',\n",
       " 'malted',\n",
       " 'heavenly',\n",
       " 'fruit',\n",
       " 'death',\n",
       " 'crust',\n",
       " 'you',\n",
       " 'very',\n",
       " 'sinful',\n",
       " 'silk',\n",
       " 'pots',\n",
       " 'potato',\n",
       " 'perfect',\n",
       " 'italian',\n",
       " 'glazed',\n",
       " 'diabetic',\n",
       " 'chocolatey',\n",
       " 'bittersweet',\n",
       " 'better',\n",
       " 'bark',\n",
       " 'yogurt',\n",
       " 'whipped',\n",
       " 'way',\n",
       " 'two',\n",
       " 'tarts',\n",
       " 'sundae',\n",
       " 'stuffed',\n",
       " 'special',\n",
       " 'roll',\n",
       " 'joy',\n",
       " 'jar',\n",
       " 'grand',\n",
       " 'famous',\n",
       " 'drop',\n",
       " 'dream',\n",
       " 'cook',\n",
       " 'chunky',\n",
       " 'breakfast',\n",
       " 'awesome',\n",
       " 'apple',\n",
       " '4',\n",
       " 'weight',\n",
       " 'w',\n",
       " 'version',\n",
       " 'up',\n",
       " 'topping',\n",
       " 'than',\n",
       " 'strawberries',\n",
       " 'smoothie',\n",
       " 'shake',\n",
       " 'popcorn',\n",
       " 'nutty',\n",
       " 'marnier',\n",
       " 'latte',\n",
       " 'heaven',\n",
       " 'ghirardelli',\n",
       " 'flour',\n",
       " 'easiest',\n",
       " 'cafe',\n",
       " 'brown',\n",
       " 'biscuits',\n",
       " 'bailey',\n",
       " 'almost',\n",
       " 'watchers',\n",
       " 'upside',\n",
       " 'toast',\n",
       " 'souffle',\n",
       " 'snickers',\n",
       " 'pops',\n",
       " 'points',\n",
       " 'on',\n",
       " 'not',\n",
       " 'gourmet',\n",
       " 'ginger',\n",
       " 'eclair',\n",
       " 'deluxe',\n",
       " 'cracker',\n",
       " 'bon',\n",
       " 'thick',\n",
       " 'texas',\n",
       " 'tea',\n",
       " 'so',\n",
       " 'sandwiches',\n",
       " 'puddings',\n",
       " 'praline',\n",
       " 'oh',\n",
       " 'new',\n",
       " 'nestle',\n",
       " 'nanaimo',\n",
       " 'mississippi',\n",
       " 'ii',\n",
       " 'fantasy',\n",
       " 'fail',\n",
       " 'crinkles',\n",
       " 'chippers',\n",
       " 'chili',\n",
       " 'cherries',\n",
       " 'berry',\n",
       " 'bakery',\n",
       " 'baked',\n",
       " 'applesauce',\n",
       " 'aka',\n",
       " 'whoopie',\n",
       " 'valentine',\n",
       " 'swirled',\n",
       " 'self',\n",
       " 'really',\n",
       " 'pretzel',\n",
       " 'pies',\n",
       " 'parfaits',\n",
       " 'nuts',\n",
       " 'mascarpone',\n",
       " 'lower',\n",
       " 'lover',\n",
       " 'liqueur',\n",
       " 'like',\n",
       " 'lighter',\n",
       " 'guinness',\n",
       " 'giant',\n",
       " 'english',\n",
       " 'chicken',\n",
       " 'brittle',\n",
       " 'blueberry',\n",
       " 'bean',\n",
       " 'angel',\n",
       " 'yum',\n",
       " 't',\n",
       " 'scratch',\n",
       " 'saucing',\n",
       " 'raw',\n",
       " 'parfait',\n",
       " 'monster',\n",
       " 'mole',\n",
       " 'king',\n",
       " 'kentucky',\n",
       " 'jumbo',\n",
       " 'individual',\n",
       " 'guilt',\n",
       " 'fluffy',\n",
       " 'flan',\n",
       " 'eggless',\n",
       " 'drink',\n",
       " 'crepes',\n",
       " 'contessa',\n",
       " 'chocolaty',\n",
       " 'chewies',\n",
       " 'chef',\n",
       " 'cashew',\n",
       " 'butterfinger',\n",
       " 'birthday',\n",
       " 'barefoot',\n",
       " 'au',\n",
       " 'thin',\n",
       " 'split',\n",
       " 'salty',\n",
       " 'ribbon',\n",
       " 'reduced',\n",
       " 'party',\n",
       " 'ooey',\n",
       " 'mallow',\n",
       " 'made',\n",
       " 'macaroon',\n",
       " 'kittencal',\n",
       " 'jam',\n",
       " 'it',\n",
       " 'high',\n",
       " 'grasshopper',\n",
       " 'granola',\n",
       " 'gift',\n",
       " 'diet',\n",
       " 'date',\n",
       " 'crme',\n",
       " 'cowboy',\n",
       " 'chai',\n",
       " 'cane',\n",
       " 'blue',\n",
       " 'batch',\n",
       " 'toblerone',\n",
       " 'time',\n",
       " 'swiss',\n",
       " 'supreme',\n",
       " 'st',\n",
       " 'snicker',\n",
       " 'secret',\n",
       " 'puff',\n",
       " 'pecans',\n",
       " 'pear',\n",
       " 'nigella',\n",
       " 'meringues',\n",
       " 'mama',\n",
       " 'malt',\n",
       " 'lovers',\n",
       " 'love',\n",
       " 'krispie',\n",
       " 'kids',\n",
       " 'instant',\n",
       " 'gingerbread',\n",
       " 'crumb',\n",
       " 'chunks',\n",
       " 'chocolat',\n",
       " 'carrot',\n",
       " 'cannoli',\n",
       " 'can',\n",
       " 'calorie',\n",
       " 'brazilian',\n",
       " 'bran',\n",
       " 'be',\n",
       " 'basic',\n",
       " 'ball',\n",
       " 'arthur',\n",
       " 'all',\n",
       " 'york',\n",
       " 'wine',\n",
       " 'whiskey',\n",
       " 'viennese',\n",
       " 'variations',\n",
       " 'trash',\n",
       " 'top',\n",
       " 'tofu',\n",
       " 'three',\n",
       " 'that',\n",
       " 'spread',\n",
       " 'snowballs',\n",
       " 'sex',\n",
       " 'ricotta',\n",
       " 'recipes',\n",
       " 'protein',\n",
       " 'outrageous',\n",
       " 'out',\n",
       " 'mother',\n",
       " 'most',\n",
       " 'mock',\n",
       " 'milky',\n",
       " 'mice',\n",
       " 'melting',\n",
       " 'martha',\n",
       " 'luscious',\n",
       " 'lawson',\n",
       " 'kitchen',\n",
       " 'incredible',\n",
       " 'icebox',\n",
       " 'i',\n",
       " 'heart',\n",
       " 'healthier',\n",
       " 'great',\n",
       " 'godiva',\n",
       " 'frappe',\n",
       " 'fast',\n",
       " 'extreme',\n",
       " 'everything',\n",
       " 'eggnog',\n",
       " 'dip',\n",
       " 'devils',\n",
       " 'delights',\n",
       " 'day',\n",
       " 'custard',\n",
       " 'crinkle',\n",
       " 'crackle',\n",
       " 'country',\n",
       " 'corn',\n",
       " 'clusters',\n",
       " 'chiffon',\n",
       " 'chex',\n",
       " 'but',\n",
       " 'buckeye',\n",
       " 'blender',\n",
       " 'bacon',\n",
       " 'aunt',\n",
       " 'alton',\n",
       " 'volcano',\n",
       " 'tunnel',\n",
       " 'treasure',\n",
       " 'toasted',\n",
       " 'thumbprints',\n",
       " 'thumbprint',\n",
       " 'sticky',\n",
       " 'starbucks',\n",
       " 'spritz',\n",
       " 'sponge',\n",
       " 'sourdough',\n",
       " 'souffles',\n",
       " 'sorbet',\n",
       " 'snowball',\n",
       " 'skor',\n",
       " 'sin',\n",
       " 'silky',\n",
       " 'scrumptious',\n",
       " 'salted',\n",
       " 'roca',\n",
       " 'ripple',\n",
       " 'real',\n",
       " 'puffs',\n",
       " 'pretzels',\n",
       " 'pineapple',\n",
       " 'picnic',\n",
       " 'pb',\n",
       " 'pastry',\n",
       " 'over',\n",
       " 'neiman',\n",
       " 'millionaires',\n",
       " 'michael',\n",
       " 'menthe',\n",
       " 'marcus',\n",
       " 'little',\n",
       " 'linda',\n",
       " 'lime',\n",
       " 'la',\n",
       " 'jack',\n",
       " 'home',\n",
       " 'fruitcake',\n",
       " 'frosty',\n",
       " 'four',\n",
       " 'five',\n",
       " 'fingers',\n",
       " 'fields',\n",
       " 'extra',\n",
       " 'dried',\n",
       " 'dollar',\n",
       " 'dirt',\n",
       " 'different',\n",
       " 'die',\n",
       " 'decadence',\n",
       " 'crumble',\n",
       " 'crowd',\n",
       " 'croissant',\n",
       " 'crisps',\n",
       " 'crisp',\n",
       " 'cornflake',\n",
       " 'cones',\n",
       " 'citrus',\n",
       " 'chess',\n",
       " 'cheesecakes',\n",
       " 'cereal',\n",
       " 'cabernet',\n",
       " 'buckeyes',\n",
       " 'boston',\n",
       " 'bons',\n",
       " 'bonbons',\n",
       " 'blackout',\n",
       " 'bits',\n",
       " 'bisquick',\n",
       " 'bananas',\n",
       " 'baileys',\n",
       " 'bag',\n",
       " 'andes',\n",
       " 'almonds',\n",
       " 'x',\n",
       " 'winning',\n",
       " 'whip',\n",
       " 'wacky',\n",
       " 'using',\n",
       " 'ultra',\n",
       " 'trail',\n",
       " 'taste',\n",
       " 'sundaes',\n",
       " 'star',\n",
       " 'south',\n",
       " 'shop',\n",
       " 'seriously',\n",
       " 'scottish',\n",
       " 'salad',\n",
       " 'safe',\n",
       " 'rolls',\n",
       " 'refrigerator',\n",
       " 'raspberries',\n",
       " 'pillows',\n",
       " 'patty',\n",
       " 'patties',\n",
       " 'o',\n",
       " 'nutter',\n",
       " 'non',\n",
       " 'nana',\n",
       " 'mountain',\n",
       " 'mounds',\n",
       " 'moon',\n",
       " 'monkey',\n",
       " 'molasses',\n",
       " 'mixed',\n",
       " 'million',\n",
       " 'milkshakes',\n",
       " 'mayonnaise',\n",
       " 'mars',\n",
       " 'marmalade',\n",
       " 'machine',\n",
       " 'logs',\n",
       " 'log',\n",
       " 'layers',\n",
       " 'kid',\n",
       " 'key',\n",
       " 'just',\n",
       " 'jean',\n",
       " 'irresistible',\n",
       " 'indulgence',\n",
       " 'hotel',\n",
       " 'hidden',\n",
       " 'hg',\n",
       " 'grilled',\n",
       " 'girl',\n",
       " 'gateau',\n",
       " 'friendly',\n",
       " 'flax',\n",
       " 'dump',\n",
       " 'daniel',\n",
       " 'dad',\n",
       " 'crescent',\n",
       " 'crackles',\n",
       " 'cola',\n",
       " 'cobbler',\n",
       " 'chocolates',\n",
       " 'chews',\n",
       " 'cal',\n",
       " 'caf',\n",
       " 'buttery',\n",
       " 'blossoms',\n",
       " 'blonde',\n",
       " 'beetroot',\n",
       " 'beet',\n",
       " 'bear',\n",
       " 'baby',\n",
       " 'b',\n",
       " 'as',\n",
       " 'apples',\n",
       " 'alice',\n",
       " 'zebra',\n",
       " 'yums',\n",
       " 'yourself',\n",
       " 'wonderful',\n",
       " 'won',\n",
       " 'wilbur',\n",
       " 'wicked',\n",
       " 'whatever',\n",
       " 'waffle',\n",
       " 'wafers',\n",
       " 'turkey',\n",
       " 'triangles',\n",
       " 'tomato',\n",
       " 'todd',\n",
       " 'thunder',\n",
       " 'this',\n",
       " 'tasty',\n",
       " 'sweetened',\n",
       " 'swedish',\n",
       " 'stewart',\n",
       " 'spring',\n",
       " 'spinach',\n",
       " 'spanish',\n",
       " 'southern',\n",
       " 'soup',\n",
       " 'soda',\n",
       " 'smooth',\n",
       " 'sink',\n",
       " 'seven',\n",
       " 'seed',\n",
       " 'scout',\n",
       " 'sauerkraut',\n",
       " 'santa',\n",
       " 'ruth',\n",
       " 'russian',\n",
       " 'royal',\n",
       " 'rolo',\n",
       " 'rolled',\n",
       " 'risotto',\n",
       " 'ring',\n",
       " 'reeses',\n",
       " 'rainbow',\n",
       " 'punch',\n",
       " 'powder',\n",
       " 'poppy',\n",
       " 'pms',\n",
       " 'petit',\n",
       " 'perfectly',\n",
       " 'pepper',\n",
       " 'pears',\n",
       " 'patrick',\n",
       " 'passion',\n",
       " 'pampered',\n",
       " 'oven',\n",
       " 'oreos',\n",
       " 'oats',\n",
       " 'never',\n",
       " 'mudslide',\n",
       " 'miss',\n",
       " 'melt',\n",
       " 'mean',\n",
       " 'me',\n",
       " 'mary',\n",
       " 'marvelous',\n",
       " 'make',\n",
       " 'lots',\n",
       " 'loaves',\n",
       " 'lite',\n",
       " 'lightened',\n",
       " 'lace',\n",
       " 'kit',\n",
       " 'killer',\n",
       " 'kat',\n",
       " 'jello',\n",
       " 'island',\n",
       " 'inside',\n",
       " 'ho',\n",
       " 'hedgehog',\n",
       " 'heath',\n",
       " 'gravy',\n",
       " 'granny',\n",
       " 'gone',\n",
       " 'goat',\n",
       " 'glossy',\n",
       " 'gelato',\n",
       " 'fun',\n",
       " 'frothy',\n",
       " 'freezer',\n",
       " 'frappuccino',\n",
       " 'forgotten',\n",
       " 'fluff',\n",
       " 'field',\n",
       " 'fabulous',\n",
       " 'elegant',\n",
       " 'eclairs',\n",
       " 'e',\n",
       " 'dutch',\n",
       " 'dreamy',\n",
       " 'doughnuts',\n",
       " 'divine',\n",
       " 'dish',\n",
       " 'di',\n",
       " 'dense',\n",
       " 'deliciously',\n",
       " 'dates',\n",
       " 'darn',\n",
       " 'crescents',\n",
       " 'creams',\n",
       " 'crazy',\n",
       " 'cranberries',\n",
       " 'cool',\n",
       " 'cooker',\n",
       " 'cooked',\n",
       " 'cocktail',\n",
       " 'coca',\n",
       " 'churros',\n",
       " 'chow',\n",
       " 'chile',\n",
       " 'chestnut',\n",
       " 'champurrado',\n",
       " 'cat',\n",
       " 'cash',\n",
       " 'carol',\n",
       " 'carob',\n",
       " 'caramels',\n",
       " 'caramelized',\n",
       " 'candies',\n",
       " 'cafeteria',\n",
       " 'c',\n",
       " 'buddha',\n",
       " 'brulee',\n",
       " 'brioche',\n",
       " 'blackberry',\n",
       " 'bite',\n",
       " 'betty',\n",
       " 'beer',\n",
       " 'beans',\n",
       " 'bavarian',\n",
       " 'aztec',\n",
       " 'award',\n",
       " 'amy',\n",
       " 'american',\n",
       " 'america',\n",
       " 'amazingly',\n",
       " 'amaretti',\n",
       " '7',\n",
       " '10',\n",
       " 'y',\n",
       " 'wreath',\n",
       " 'winter',\n",
       " 'whites',\n",
       " 'walnuts',\n",
       " 'use',\n",
       " 'twix',\n",
       " 'twist',\n",
       " 'turnovers',\n",
       " 'treat',\n",
       " 'torta',\n",
       " 'tipsy',\n",
       " 'tiger',\n",
       " 'tex',\n",
       " 'test',\n",
       " 'tender',\n",
       " 'take',\n",
       " 'swirls',\n",
       " 'surprises',\n",
       " 'stuff',\n",
       " 'stove',\n",
       " 'sticks',\n",
       " 'step',\n",
       " 'steamed',\n",
       " 'square',\n",
       " 'splenda',\n",
       " 'sommer',\n",
       " 'smores',\n",
       " 'smoothies',\n",
       " 'smith',\n",
       " 'small',\n",
       " 'slow',\n",
       " 'slices',\n",
       " 'slab',\n",
       " 'six',\n",
       " 'sinfully',\n",
       " 'simply',\n",
       " 'shots',\n",
       " 'shortcakes',\n",
       " 'sherry',\n",
       " 'shaped',\n",
       " 'scented',\n",
       " 'savannah',\n",
       " 'satin',\n",
       " 'saltine',\n",
       " 'rugelach',\n",
       " 'roof',\n",
       " 'robert',\n",
       " 'ripe',\n",
       " 'richest',\n",
       " 'restaurant',\n",
       " 'redford',\n",
       " 'realtor',\n",
       " 'ray',\n",
       " 'raisins',\n",
       " 'rachel',\n",
       " 'rachael',\n",
       " 'quesadillas',\n",
       " 'puppy',\n",
       " 'pts',\n",
       " 'pt',\n",
       " 'pour',\n",
       " 'possum',\n",
       " 'poke',\n",
       " 'point',\n",
       " 'pinwheel',\n",
       " 'pine',\n",
       " 'pick',\n",
       " 'philly',\n",
       " 'pepperidge',\n",
       " 'peanutty',\n",
       " 'paradise',\n",
       " 'panna',\n",
       " 'p',\n",
       " 'original',\n",
       " 'only',\n",
       " 'oil',\n",
       " 'ohio',\n",
       " 'off',\n",
       " 'oamc',\n",
       " 'nuggets',\n",
       " 'next',\n",
       " 'mystery',\n",
       " 'muffin',\n",
       " 'muddy',\n",
       " 'mouth',\n",
       " 'morsel',\n",
       " 'montezuma',\n",
       " 'mochaccino',\n",
       " 'minty',\n",
       " 'mints',\n",
       " 'mimi',\n",
       " 'millionaire',\n",
       " 'mexi',\n",
       " 'mex',\n",
       " 'merry',\n",
       " 'melts',\n",
       " 'martini',\n",
       " 'marie',\n",
       " 'mango',\n",
       " 'mandarin',\n",
       " 'mamie',\n",
       " 'malteser',\n",
       " 'mahogany',\n",
       " 'magnificent',\n",
       " 'madeleines',\n",
       " 'luby',\n",
       " 'loves',\n",
       " 'lou',\n",
       " 'level',\n",
       " 'less',\n",
       " 'lee',\n",
       " 'lait',\n",
       " 'knock',\n",
       " 'kind',\n",
       " 'kate',\n",
       " 'k',\n",
       " 'jumbles',\n",
       " 'julia',\n",
       " 'jerry',\n",
       " 'jelly',\n",
       " 'jane',\n",
       " 'jaffa',\n",
       " 'intensely',\n",
       " 'ingredients',\n",
       " 'ingredient',\n",
       " 'incredibly',\n",
       " 'hungarian',\n",
       " 'hugs',\n",
       " 'honeycomb',\n",
       " 'harvest',\n",
       " 'guiltless',\n",
       " 'green',\n",
       " 'greek',\n",
       " 'grammy',\n",
       " 'grain',\n",
       " 'grab',\n",
       " 'gorp',\n",
       " 'gorgeous',\n",
       " 'goober',\n",
       " 'golden',\n",
       " 'gold',\n",
       " 'giving',\n",
       " 'get',\n",
       " 'fudgies',\n",
       " 'fridge',\n",
       " 'fresh',\n",
       " 'frangelico',\n",
       " 'fondant',\n",
       " 'florentines',\n",
       " 'florentine',\n",
       " 'flake',\n",
       " 'fit',\n",
       " 'festive',\n",
       " 'ferrero',\n",
       " 'fashion',\n",
       " 'family',\n",
       " 'fair',\n",
       " 'eric',\n",
       " 'energy',\n",
       " 'enchiladas',\n",
       " 'emeril',\n",
       " 'ecstasy',\n",
       " 'drumstick',\n",
       " 'dressing',\n",
       " 'dreams',\n",
       " 'doubletree',\n",
       " 'died',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, name_processor, ingredient_processor, units):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    self.name_processor = name_processor\n",
    "    self.ingredient_processor = ingredient_processor\n",
    "\n",
    "    self.name_vocab_size = name_processor.vocabulary_size()\n",
    "    self.ingredient_vocab_size = ingredient_processor.vocabulary_size()\n",
    "    \n",
    "    self.units = units\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.name_embedding = tf.keras.layers.Embedding(self.name_vocab_size, units, mask_zero=True)\n",
    "    self.ingredient_embedding = tf.keras.layers.Embedding(self.ingredient_vocab_size, units, mask_zero=True)\n",
    "\n",
    "    self.concat = Concatenate()\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding((self.name_vocab_size + self.ingredient_vocab_size), units, mask_zero=True)\n",
    "\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "        merge_mode='sum',\n",
    "        layer=tf.keras.layers.GRU(units,\n",
    "                            # Return the sequence and state\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "  def call(self, ingredients, names):\n",
    "\n",
    "    shape_checker = ShapeChecker()\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding vector for each token.\n",
    "    x1 = self.name_embedding(ingredients)\n",
    "    x2 = self.ingredient_embedding(names)\n",
    "\n",
    "    #x = self.concat([x1, x2])\n",
    "    x = x1\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "   \n",
    "    # 3. The GRU processes the sequence of embeddings.\n",
    "    x = self.rnn(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "  \n",
    "    # 4. Returns the new sequence of embeddings.\n",
    "    return x\n",
    "\n",
    "  def convert_input(self, ingredients, names):\n",
    "    ingredients = tf.convert_to_tensor(ingredients)\n",
    "    names = tf.convert_to_tensor(names)\n",
    "    \n",
    "    names = self.name_processor(names)#.to_tensor()\n",
    "    ingredients = self.ingredient_processor(ingredients)#.to_tensor()\n",
    "    \n",
    "    context = self(ingredients, names)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(name_vectorize_layer, ingredient_vectorize_layer, UNITS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'get the sensation  brownies'\n",
      "tf.Tensor([ 975   33 1876    7], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (ex_names) in name_dataset.take(1):\n",
    "  print(ex_names.numpy()) \n",
    "ex_names_tok = name_vectorize_layer(ex_names)\n",
    "print(ex_names_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'butter, sugar, vanilla, eggs, all-purpose_flour, baking_cocoa, baking_powder, salt, miniature_peppermint_patties'\n",
      "tf.Tensor([2 3 6 5], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (ex_ingredients) in ingredient_dataset.take(1):\n",
    "  print(ex_ingredients.numpy()) \n",
    "\n",
    "ex_ingredients_tok = ingredient_vectorize_layer(ex_ingredients)\n",
    "print(ex_ingredients_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'preheat oven to 350 degrees <STEP> grease 13 x 9 baking pan <STEP> in a large bowl , whisk together butter , sugar and vanilla <STEP> add eggs and stir until well combined <STEP> combine dry ingredients and blend well with wet ingredients <STEP> reserve 2 cups of batter and set aside <STEP> spread remaining batter in prepared pan <STEP> arrange peppermint patties in a single layer over batter , about 1 / 2 inch apart <STEP> carefully spread reserved 2 cups of batter on top <STEP> bake for 50- 55 minutes or until brownies begin to pull away from sides of pan <STEP> cool completely on wire rack and cut into squares\\n'\n",
      "tf.Tensor(\n",
      "[  54   30    7   68   77    2  128  304  217  190   28   20    2    5\n",
      "    6   52   21   85   49   23   18    3   36    2   15   56    3   22\n",
      "    8   38  124    2   41   96   59    3  122   38    9  332   59    2\n",
      "  642   34  136   11   66    3   44   91    2   61   75   66    5  112\n",
      "   20    2  477  707 1443    5    6  771  135   26   66   55   24   34\n",
      "  131  237    2  305   61  468   34  136   11   66   17   46    2   29\n",
      "   12  465  836   13   16    8  209  557    7  588  463   42  152   11\n",
      "   20    2   25   74   17  101   86    3   83   14  213    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(400,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (ex_steps) in step_dataset.take(1):\n",
    "  print(ex_steps.numpy()) \n",
    "ex_steps_tok = step_vectorize_layer(ex_steps)\n",
    "print(ex_steps_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 256, 256), dtype=float32, numpy=\n",
       "array([[[ 0.02010092,  0.01672063,  0.01243637, ..., -0.04673108,\n",
       "         -0.01720304,  0.01636258],\n",
       "        [ 0.02342843,  0.0121458 ,  0.01253262, ..., -0.0494245 ,\n",
       "         -0.01391483,  0.02257711],\n",
       "        [ 0.02460716,  0.00909669,  0.01141806, ..., -0.05020532,\n",
       "         -0.01017253,  0.0267278 ],\n",
       "        ...,\n",
       "        [ 0.02240387,  0.00014399,  0.0048982 , ..., -0.04343624,\n",
       "         -0.00283831,  0.03283363],\n",
       "        [ 0.02050241, -0.00418426,  0.00397169, ..., -0.03881026,\n",
       "         -0.00019266,  0.03186321],\n",
       "        [ 0.0166775 , -0.01035845,  0.0025769 , ..., -0.02918759,\n",
       "          0.00606153,  0.0303477 ]],\n",
       "\n",
       "       [[ 0.02010092,  0.01672063,  0.01243637, ..., -0.04673108,\n",
       "         -0.01720304,  0.01636258],\n",
       "        [ 0.02342843,  0.0121458 ,  0.01253262, ..., -0.0494245 ,\n",
       "         -0.01391483,  0.02257711],\n",
       "        [ 0.02460716,  0.00909669,  0.01141806, ..., -0.05020532,\n",
       "         -0.01017253,  0.0267278 ],\n",
       "        ...,\n",
       "        [ 0.02240387,  0.00014399,  0.0048982 , ..., -0.04343624,\n",
       "         -0.00283831,  0.03283363],\n",
       "        [ 0.02050241, -0.00418426,  0.00397169, ..., -0.03881026,\n",
       "         -0.00019266,  0.03186321],\n",
       "        [ 0.0166775 , -0.01035845,  0.0025769 , ..., -0.02918759,\n",
       "          0.00606153,  0.0303477 ]],\n",
       "\n",
       "       [[ 0.02010092,  0.01672063,  0.01243637, ..., -0.04673108,\n",
       "         -0.01720304,  0.01636258],\n",
       "        [ 0.02342843,  0.0121458 ,  0.01253262, ..., -0.0494245 ,\n",
       "         -0.01391483,  0.02257711],\n",
       "        [ 0.02460716,  0.00909669,  0.01141806, ..., -0.05020532,\n",
       "         -0.01017253,  0.0267278 ],\n",
       "        ...,\n",
       "        [ 0.02240387,  0.00014399,  0.0048982 , ..., -0.04343624,\n",
       "         -0.00283831,  0.03283363],\n",
       "        [ 0.02050241, -0.00418426,  0.00397169, ..., -0.03881026,\n",
       "         -0.00019266,  0.03186321],\n",
       "        [ 0.0166775 , -0.01035845,  0.0025769 , ..., -0.02918759,\n",
       "          0.00606153,  0.0303477 ]],\n",
       "\n",
       "       [[ 0.02010092,  0.01672063,  0.01243637, ..., -0.04673108,\n",
       "         -0.01720304,  0.01636258],\n",
       "        [ 0.02342843,  0.0121458 ,  0.01253262, ..., -0.0494245 ,\n",
       "         -0.01391483,  0.02257711],\n",
       "        [ 0.02460716,  0.00909669,  0.01141806, ..., -0.05020532,\n",
       "         -0.01017253,  0.0267278 ],\n",
       "        ...,\n",
       "        [ 0.02240387,  0.00014399,  0.0048982 , ..., -0.04343624,\n",
       "         -0.00283831,  0.03283363],\n",
       "        [ 0.02050241, -0.00418426,  0.00397169, ..., -0.03881026,\n",
       "         -0.00019266,  0.03186321],\n",
       "        [ 0.0166775 , -0.01035845,  0.0025769 , ..., -0.02918759,\n",
       "          0.00606153,  0.0303477 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_name = name_dataset.take(1)\n",
    "sample_ingredient = ingredient_dataset.take(1)\n",
    "ex_context = encoder(ex_ingredients_tok, ex_names_tok)\n",
    "#ex_context = encoder.convert_input(ex_ingredient_tok, ex_name_tok)\n",
    "ex_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "    \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    shape_checker(attn_scores, 'batch t s')\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling layer 'cross_attention_4' (type CrossAttention).\n\nCan't parse shape with different number of dimensions: batch t units (400, 256)\n\nCall arguments received by layer 'cross_attention_4' (type CrossAttention):\n  • x=tf.Tensor(shape=(400, 256), dtype=float32)\n  • context=tf.Tensor(shape=(4, 256, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m embed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(step_vectorize_layer\u001b[39m.\u001b[39mvocabulary_size(),\n\u001b[0;32m      5\u001b[0m                                   output_dim\u001b[39m=\u001b[39mUNITS, mask_zero\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbliblablub\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m ex_tar_embed \u001b[39m=\u001b[39m embed(ex_steps_tok)\n\u001b[1;32m----> 8\u001b[0m result \u001b[39m=\u001b[39m attention_layer(ex_tar_embed, ex_context)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mContext sequence, shape (batch, s, units): \u001b[39m\u001b[39m{\u001b[39;00mex_context\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget sequence, shape (batch, t, units): \u001b[39m\u001b[39m{\u001b[39;00mex_tar_embed\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\salha\\studium\\NLP\\projektabeit_nlp_2\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[62], line 11\u001b[0m, in \u001b[0;36mCrossAttention.call\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, context):\n\u001b[0;32m     10\u001b[0m   shape_checker \u001b[39m=\u001b[39m ShapeChecker()\n\u001b[1;32m---> 11\u001b[0m   shape_checker(x, \u001b[39m'\u001b[39;49m\u001b[39mbatch t units\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m   shape_checker(context, \u001b[39m'\u001b[39m\u001b[39mbatch s units\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m   attn_output, attn_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmha(\n\u001b[0;32m     16\u001b[0m       query\u001b[39m=\u001b[39mx,\n\u001b[0;32m     17\u001b[0m       value\u001b[39m=\u001b[39mcontext,\n\u001b[0;32m     18\u001b[0m       return_attention_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m, in \u001b[0;36mShapeChecker.__call__\u001b[1;34m(self, tensor, names, broadcast)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m      8\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m parsed \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39;49mparse_shape(tensor, names)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m name, new_dim \u001b[39min\u001b[39;00m parsed\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m   old_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshapes\u001b[39m.\u001b[39mget(name, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\salha\\studium\\NLP\\projektabeit_nlp_2\\venv\\lib\\site-packages\\einops\\einops.py:573\u001b[0m, in \u001b[0;36mparse_shape\u001b[1;34m(x, pattern)\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt parse shape with this number of dimensions: \u001b[39m\u001b[39m{pattern}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{shape}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    571\u001b[0m                 pattern\u001b[39m=\u001b[39mpattern, shape\u001b[39m=\u001b[39mshape))\n\u001b[0;32m    572\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 573\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt parse shape with different number of dimensions: \u001b[39m\u001b[39m{pattern}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{shape}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    574\u001b[0m             pattern\u001b[39m=\u001b[39mpattern, shape\u001b[39m=\u001b[39mshape))\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m exp\u001b[39m.\u001b[39mhas_ellipsis:\n\u001b[0;32m    576\u001b[0m     ellipsis_idx \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39mcomposition\u001b[39m.\u001b[39mindex(_ellipsis)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception encountered when calling layer 'cross_attention_4' (type CrossAttention).\n\nCan't parse shape with different number of dimensions: batch t units (400, 256)\n\nCall arguments received by layer 'cross_attention_4' (type CrossAttention):\n  • x=tf.Tensor(shape=(400, 256), dtype=float32)\n  • context=tf.Tensor(shape=(4, 256, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(step_vectorize_layer.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True, name=\"bliblablub\")\n",
    "\n",
    "ex_tar_embed = embed(ex_steps_tok)\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                               units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "  # 1. Lookup the embeddings\n",
    "  x = self.embedding(x)\n",
    "\n",
    "  # 2. Process the target sequence.\n",
    "  x, state = self.rnn(x, initial_state=state)\n",
    "  # 3. Use the RNN output as the query for the attention over the context.\n",
    "  x = self.attention(x, context)\n",
    "  self.last_attention_weights = self.attention.last_attention_weights\n",
    "\n",
    "  # Step 4. Generate logit predictions for the next token.\n",
    "  logits = self.output_layer(x)\n",
    "\n",
    "  if return_state:\n",
    "    return logits, state\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(step_vectorize_layer, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'decoder_3' (type Decoder).\n\nInput 0 of layer \"gru_7\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (400, 256)\n\nCall arguments received by layer 'decoder_3' (type Decoder):\n  • context=tf.Tensor(shape=(4, 512, 256), dtype=float32)\n  • x=tf.Tensor(shape=(400,), dtype=int64)\n  • state=None\n  • return_state=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logits \u001b[39m=\u001b[39m decoder(ex_context, ex_steps_tok)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mencoder output shape: (batch, s, units) \u001b[39m\u001b[39m{\u001b[39;00mex_context\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput target tokens shape: (batch, t) \u001b[39m\u001b[39m{\u001b[39;00mex_steps_tok\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\salha\\studium\\NLP\\projektabeit_nlp_2\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[97], line 10\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(self, context, x, state, return_state)\u001b[0m\n\u001b[0;32m      7\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m      9\u001b[0m \u001b[39m# 2. Process the target sequence.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m x, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, initial_state\u001b[39m=\u001b[39;49mstate)\n\u001b[0;32m     11\u001b[0m \u001b[39m# 3. Use the RNN output as the query for the attention over the context.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(x, context)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'decoder_3' (type Decoder).\n\nInput 0 of layer \"gru_7\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (400, 256)\n\nCall arguments received by layer 'decoder_3' (type Decoder):\n  • context=tf.Tensor(shape=(4, 512, 256), dtype=float32)\n  • x=tf.Tensor(shape=(400,), dtype=int64)\n  • state=None\n  • return_state=False"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_steps_tok)\n",
    "\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_steps_tok.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "  embedded = self.embedding(start_tokens)\n",
    "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "  words = self.id_to_word(tokens)\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "  logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "\n",
    "  if temperature == 0.0:\n",
    "    next_token = tf.argmax(logits, axis=-1)\n",
    "  else:\n",
    "    logits = logits[:, -1, :]/temperature\n",
    "    next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "  # If a sequence produces an `end_token`, set it `done`\n",
    "  done = done | (next_token == self.end_token)\n",
    "  # Once a sequence is done it only produces 0-padding.\n",
    "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "  return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'browning amazing owls dumping yogurt brim void metric topside muscovado',\n",
       "       b'camp lrg meats rearrange remelt key 2832 halves reasons fondue',\n",
       "       b'criss less frog twist lightlygreased nestling floured minuted fallon slighly'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, units,  name_processor, ingredient_processor, target_text_processor):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder( name_processor, ingredient_processor, units)\n",
    "    decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, ingredients, names, steps):\n",
    "    \n",
    "    context = self.encoder.convert_input(ingredients, names)\n",
    "    logits = self.decoder(context, steps)\n",
    "\n",
    "    #TODO(b/250038731): remove this\n",
    "    try:\n",
    "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Translator(UNITS, name_vectorize_layer, ingredient_vectorize_layer, step_vectorize_layer)\n\u001b[0;32m      3\u001b[0m logits \u001b[39m=\u001b[39m model(ex_ingredient_tok, ex_name_tok, ex_name_tok)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mContext tokens, shape: (batch, s, units) \u001b[39m\u001b[39m{\u001b[39;00mex_name_tok\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Translator' is not defined"
     ]
    }
   ],
   "source": [
    "model = Translator(UNITS, name_vectorize_layer, ingredient_vectorize_layer, step_vectorize_layer)\n",
    "\n",
    "logits = model(ex_ingredient_tok, ex_name_tok, ex_name_tok)\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_name_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "700212c39ba4d9c4ad4a0eeefea16618b4c85aca31676250ae9aeb57c9b998d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
